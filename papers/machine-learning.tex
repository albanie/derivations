\chapter{Machine Learning} \label{chap:ml}

\subsection{Understanding deep learning requires rethinking generalisation \cite{zhang2016understanding}, Theorem 1}

Link to paper: \url{https://arxiv.org/abs/1611.03530}. \\

This paper contains a nice theorem stating that a two-layer neural network containing a ReLU is capable of mapping $n$ samples $\{\mathbf{x}_i\}_{i=1}^n$ where $\mathbf{x}_i \in \real^d$ to $n$ arbitrary labels $\{y_i\}_{i=1}^n$ using only $2n + d$ parameters.  In the paper, it is stated like this:

\papersays{We say that a neural network $C$ can represent any function of a sample of size $n$ in $d$ dimensions if for every sample $S \subset \real^d$ with $|S| = n$ and every function $f: S \to \real $, there exists a setting of the weights of $C$ such that $C(x) = f(x)$ for every $x \in S$. \\
\textbf{Theorem 1.} There exists a two-layer neural network with ReLU activations and $2n + d$ weights that can represent any function on a sample of size $n$ in $d$ dimensions.
}

They also give an outline of a proof for this theorem in the appendix. It begins with this lemma:

\papersays{\textbf{Lemma 1.} For any two interleaving sequences of $n$ real numbers $b_1 < x_1 < b_2 < x_2 < \dots < b_n < x_n$, the $n \times n $ matrix $A = [\max \{x_i - b_j, 0 \}]_{ij}$ has full rank.  Its smallest eigenvalue is $\min_i x_i - b_i$.}

The proof (given in the paper) of this can be seen from writing out the matrix definition and using the fact that $x_i > b_i$:

\begin{align}
A = \max \Bigg(
\begin{bmatrix}
  x_1 - b_1 & x_1 - b_2 & \hdots & x_1 - b_n \\
  x_2 - b_1 & x_2 - b_2 & \hdots & x_2 - b_n \\
  \vdots & \vdots & \ddots & \vdots \\
  x_n - b_1 & x_n - b_2 & \hdots & x_n - b_n \\
\end{bmatrix}
, 0 \Bigg) = 
\begin{bmatrix}
  x_1 - b_1 & 0 & \hdots & 0 \\
  x_2 - b_1 & x_2 - b_2 & \hdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  x_n - b_1 & x_n - b_2 & \hdots & x_n - b_n \\
\end{bmatrix}
\end{align}

So $A$ is lower-triangular, and therefore full-rank (so invertible).  The eigenvalues of a lower-triangular matrix matrix lie on the diagonal, so the smallest eigenvalue is the smallest element on the diagonal, which is $\min_i (x_i - b_i)$.

Next the paper gives the following proof of Theorem 1.  For weights $\mathbf{w}, \mathbf{b} \in \real^n$ and $\mathbf{a} \in \real^d$, consider the function $c: \real^n \to \real$, 

\papersays{
\begin{align}
c(\mathbf{x}) = \sum_{j=1} w_j \max \{ \langle \mathbf{a}, \mathbf{x} \rangle - b_j , 0\} \label{eqn:target-fn}
\end{align}
}

\papersays{It is now easy to see that $c$ can be expressed by a depth $2$ network with ReLU activations.} \\

The questions I had here were: (1) How do we pick the values of $\ba$ to get the desired behaviour in matrix $A$ and (2) What does the shape of the network look like? \\

Let's first work out (1), i.e. how to construct the vector $\ba \in \real^d$. We need to project each $d$-dimensional sample $\xx_i$ to a scalar $x_i$ such that we get a sequence $x_1 < x_2 < \dots < x_n$. Once we've done this, we can just choose n  scalar values $\{b_i: i \in \{1, \dots, n\}\}$ by picking each $b_i \in (x_{i-1}, x_i)$ to get the interleaved sequences.  One way to do the projection is to first arrange the samples $\{ x_i \}_{i=1}^n$ so that they are ordered in increasing order lexicographically (i.e. order the element by the first dimension, then break any ties using the second dimension, then the third etc.).  Then take $k$ to be the maximum absolute scalar value that appears in any of the dimensions of any of the samples ($k = \max_s \max_j |x_s^j|$, where $j$ indexes the dimension and $s$ indexes the sample number). Then we can define $\ba = (k^0, \dots, k^{d-1})^T \in \real^d$, so that $x_i = \ba^\xx_i$ produces an increasing sequence.

For (2), define a vector $\mathbf{y} \in \real^n$ containing the target value for each sample $\mathbf{x}_i$ (i.e. $y_i = f(\mathbf{x}_i)$).  We want to build a function $c$ that maps each input sample $\mathbf{x}_i$ to the corresponding element $y_i$.  We know from \textbf{Lemma 1} that $A$ has full rank, so we know that there is a unique solution to the equation $A \ww = \mathbf{y}$, given by $\ww = A^{-1}\mathbf{y}$. We are now ready to define our network with two linear layers and a ReLU as follows:

% Based on Andrea's diagrams

\begin{center}
\begin{tikzpicture}[auto, node distance=2cm]
\node (x0)  [data] {$\xx_i^{(0)}\in\real^d$};
\node (f1) [block,right of=x0]{$f_1$};
\node (f2) [block,right of=f1,node distance=3.5cm]{ReLU};
%\node (dots) [right of=f2]{...};
\node (fL) [block,right of=f2,node distance=3.5cm]{$f_2$};
\node (w1) [data, below of=f1] {$\hat{\ba} \in \real^{d \times n}, \mathbf{b} \in \real^n$};
%\node (w2) [data, below of=f2] {$\ww_2$};
\node (wL) [data, below of=fL] {$\ww \in \real^n$};
\node (xL) [data, right of=fL] {$c(\xx)\in\real$};
\draw [to] (x0.east) -- (f1.west) {};
\draw [to] (f1.east) -- node {$\xx_i^{(1)} \in \real^n$} (f2.west);
\draw [to] (f2.east) -- node {$\xx_i^{(2)} \in \real^n$} (fL.west) {};
%\draw [to] (dots.east) -- node {$\xx_{L-1}$} (fL.west) {};{
\draw [to] (fL.east) -- (xL.west) {};
\draw [to] (w1.north) -- (f1.south) {};
%\draw [to] (w2.north) -- (f2.south) {};
\draw [to] (wL.north) -- (fL.south) {};
\end{tikzpicture}
\end{center}

Here the superscript $\xx_i^{(j)}$ just means the output of the $j^{\text{th}}$ layer for input sample $\xx_i$. Here, $f_1(\xx) = \hat{\ba}^T \xx - \bb$ and $f_2(\xx) = \ww^T \xx$.  Although the shape of the matrix $\hat{\ba}$ is $d \times n$, the columns are simply repeats of the vector $\ba$ i.e. $\hat{\ba} = \ba \bone_n^T$ (where we have used $\bone_n \in \real^n$ to denote a column of ones), so that $f_1$ only requires $d + n$, rather than $nd + n$ parameters.  We can now write the function $c(\xx)$ defined in Eqn.~\ref{eqn:target-fn} as:

\begin{align}
c(\xx) = (f_2 \circ \text{ReLU} \circ f_1) (\xx) = \ww^T(\text{ReLU}(\hat{\ba}^T \xx - \mathbf{b}))
\end{align}
